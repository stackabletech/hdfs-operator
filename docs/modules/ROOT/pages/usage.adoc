= Usage

Since Apache Hdfs is installed in high-availability mode, an Apache Zookeeper cluster is required to coordinate the active/passive namenode.

Install the Stackable Zookeeper operator and an Apache Zookeeper cluster like this:

[source,bash]
----
helm install zookeeper-operator stackable/zookeeper-operator
cat <<EOF | kubectl apply -f -
apiVersion: zookeeper.stackable.tech/v1alpha1
kind: ZookeeperCluster
metadata:
  name: simple-zk
spec:
  image:
    productVersion: 3.8.0
    stackableVersion: 0.9.0
  servers:
    roleGroups:
      default:
        replicas: 3
---
apiVersion: zookeeper.stackable.tech/v1alpha1
kind: ZookeeperZnode
metadata:
  name: simple-znode
spec:
  clusterRef:
    name: simple-zk
    namespace: default
EOF
----

Once a Zookeeper cluster and the operator are up and running, you can create an Apache HDFS cluster like shown below.
Please note that the version you need to specify is not only the version of Hadoop which you want to roll out, but has to be amended with a Stackable version as shown.
This Stackable version is the version of the underlying container image which is used to execute the processes.
For a list of available versions please check our https://repo.stackable.tech/#browse/browse:docker:v2%2Fstackable%2Fhadoop%2Ftags[image registry].
It should generally be safe to simply use the latest image version that is available.

[source,bash]
----
cat <<EOF | kubectl apply -f -
apiVersion: hdfs.stackable.tech/v1alpha1
kind: HdfsCluster
metadata:
  name: simple
spec:
  image:
    productVersion: 3.3.4
    stackableVersion: 0.3.0
  zookeeperConfigMapName: simple-znode
  dfsReplication: 3
  nameNodes:
    roleGroups:
      default:
        replicas: 2
  dataNodes:
    roleGroups:
      default:
        replicas: 3
  journalNodes:
    roleGroups:
      default:
        replicas: 3
EOF
----

IMPORTANT: When scaling namenodes up, make sure to increase the replica count only by one and not more nodes at once.

== Monitoring

The managed HDFS instances are automatically configured to export Prometheus metrics. See
xref:home:operators:monitoring.adoc[] for more details.

== Log aggregation

The logs can be forwarded to a Vector log aggregator by providing a discovery
ConfigMap for the aggregator and by enabling the log agent:

[source,yaml]
----
spec:
  vectorAggregatorConfigMapName: vector-aggregator-discovery
  nameNodes:
    config:
      logging:
        enableVectorAgent: true
  dataNodes:
    config:
      logging:
        enableVectorAgent: true
  journalNodes:
    config:
      logging:
        enableVectorAgent: true
----

Further information on how to configure logging, can be found in
xref:home:concepts:logging.adoc[].

== Configuration & Environment Overrides

The cluster definition also supports overriding configuration properties and environment variables, either per role or per role group, where the more specific override (role group) has precedence over the less specific one (role).

IMPORTANT: Overriding certain properties can lead to faulty clusters. In general this means, do not change ports, hostnames or properties related to data dirs, high-availability or security.

=== Configuration Properties

For a role or role group, at the same level of `config`, you can specify `configOverrides` for the `hdfs-site.xml` and `core-site.xml`. For example, if you want to set additional properties on the namenode servers, adapt the `nameNodes` section of the cluster resource like so:

[source,yaml]
----
nameNodes:
  roleGroups:
    default:
      config: [...]
      configOverrides:
        core-site.xml:
          fs.trash.interval: "5"
        hdfs-site.xml:
          dfs.namenode.num.checkpoints.retained: "3"
      replicas: 2
----

Just as for the `config`, it is possible to specify this at role level as well:

[source,yaml]
----
nameNodes:
  configOverrides:
    core-site.xml:
      fs.trash.interval: "5"
    hdfs-site.xml:
      dfs.namenode.num.checkpoints.retained: "3"
  roleGroups:
    default:
      config: [...]
      replicas: 2
----

All override property values must be strings. The properties will be formatted and escaped correctly into the XML file.

For a full list of configuration options we refer to the Apache Hdfs documentation for https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml[hdfs-site.xml] and https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml[core-site.xml]


=== Environment Variables

In a similar fashion, environment variables can be (over)written. For example per role group:

[source,yaml]
----
nameNodes:
  roleGroups:
    default:
      config: {}
      envOverrides:
        MY_ENV_VAR: "MY_VALUE"
      replicas: 1
----

or per role:

[source,yaml]
----
nameNodes:
  envOverrides:
    MY_ENV_VAR: "MY_VALUE"
  roleGroups:
    default:
      config: {}
      replicas: 1
----

IMPORTANT: Some environment variables will be overriden by the operator and cannot be set manually by the user. These are `HADOOP_HOME`, `HADOOP_CONF_DIR`, `POD_NAME` and `ZOOKEEPER`.

=== Storage for data volumes

You can mount volumes where data is stored by specifying https://kubernetes.io/docs/concepts/storage/persistent-volumes[PersistentVolumeClaims] for each individual role group:

[source,yaml]
----
dataNodes:
  roleGroups:
    default:
      config:
        resources:
          storage:
            data:
              capacity: 128Gi
----

In the above example, all data nodes in the default group will store data (the location of `dfs.datanode.name.dir`) on a `128Gi` volume.

By default, in case nothing is configured in the custom resource for a certain role group, each Pod will have a `5Gi` large volume mount for the data location.

==== Multiple storage volumes

Datanodes can have multiple disks attached to increase the storage size as well as speed.
They can be of different type, e.g. HDDs or SSDs.

You can configure multiple [PersistentVolumeClaims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) (PVCs) for the datanodes as follows:

[source,yaml]
----
dataNodes:
  roleGroups:
    default:
      config:
        resources:
          storage:
            data: # We need to overwrite the data pvcs coming from the default value
              count: 0
            my-disks:
              count: 3
              capacity: 12Ti
              hdfsStorageType: Disk
            my-ssds:
              count: 2
              capacity: 5Ti
              storageClass: premium-ssd
              hdfsStorageType: SSD
----

This will create the following PVCs:

1. `my-disks-hdfs-datanode-default-0` (12Ti)
2. `my-disks-1-hdfs-datanode-default-0` (12Ti)
3. `my-disks-2-hdfs-datanode-default-0` (12Ti)
4. `my-ssds-hdfs-datanode-default-0` (5Ti)
5. `my-ssds-1-hdfs-datanode-default-0` (5Ti)

By configuring and using a dedicated https://kubernetes.io/docs/concepts/storage/storage-classes/[StorageClass] you can configure your HDFS to use local disks attached to Kubernetes nodes.

[NOTE]
====
You might need to re-create the StatefulSet to apply the new PVC configuration because of https://github.com/kubernetes/kubernetes/issues/68737[this Kubernetes issue].
You can delete the StatefulSet using `kubectl delete sts --cascade=false <statefulset>`.
The hdfs-operator will re-create the StatefulSet automatically.
====

=== Resource Requests

// The "nightly" version is needed because the "include" directive searches for
// files in the "stable" version by default.
// TODO: remove the "nightly" version after the next platform release (current: 22.09)
include::nightly@home:concepts:stackable_resource_requests.adoc[]

If no resource requests are configured explicitly, the HDFS operator uses the following defaults:

[source,yaml]
----
dataNodes:
  roleGroups:
    default:
      config:
        resources:
          cpu:
            max: '4'
            min: '100m'
          storage:
            data:
              capacity: 2Gi
----
