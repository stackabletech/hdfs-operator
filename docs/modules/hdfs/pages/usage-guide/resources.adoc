= Resources

== Storage for data volumes

You can mount volumes where data is stored by specifying https://kubernetes.io/docs/concepts/storage/persistent-volumes[PersistentVolumeClaims] for each individual role group:

[source,yaml]
----
dataNodes:
  roleGroups:
    default:
      config:
        resources:
          storage:
            data:
              capacity: 128Gi
----

In the above example, all DataNodes in the default group will store data (the location of `dfs.datanode.name.dir`) on a `128Gi` volume.

By default, in case nothing is configured in the custom resource for a certain role group, each Pod will have a `5Gi` large volume mount for the data location.

=== Multiple storage volumes

Datanodes can have multiple disks attached to increase the storage size as well as speed.
They can be of different type, e.g. HDDs or SSDs.

You can configure multiple https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims[PersistentVolumeClaims] (PVCs) for the datanodes as follows:

[source,yaml]
----
dataNodes:
  roleGroups:
    default:
      config:
        resources:
          storage:
            data: # We need to overwrite the data pvcs coming from the default value
              count: 0
            my-disks:
              count: 3
              capacity: 12Ti
              hdfsStorageType: Disk
            my-ssds:
              count: 2
              capacity: 5Ti
              storageClass: premium-ssd
              hdfsStorageType: SSD
----

This will create the following PVCs:

1. `my-disks-hdfs-datanode-default-0` (12Ti)
2. `my-disks-1-hdfs-datanode-default-0` (12Ti)
3. `my-disks-2-hdfs-datanode-default-0` (12Ti)
4. `my-ssds-hdfs-datanode-default-0` (5Ti)
5. `my-ssds-1-hdfs-datanode-default-0` (5Ti)

By configuring and using a dedicated https://kubernetes.io/docs/concepts/storage/storage-classes/[StorageClass] you can configure your HDFS to use local disks attached to Kubernetes nodes.

[NOTE]
====
You might need to re-create the StatefulSet to apply the new PVC configuration because of https://github.com/kubernetes/kubernetes/issues/68737[this Kubernetes issue].
You can delete the StatefulSet using `kubectl delete sts --cascade=false <statefulset>`.
The hdfs-operator will re-create the StatefulSet automatically.
====

== Resource Requests

include::home:concepts:stackable_resource_requests.adoc[]

If no resource requests are configured explicitly, the HDFS operator uses the following defaults:

[source,yaml]
----
dataNodes:
  roleGroups:
    default:
      config:
        resources:
          cpu:
            max: '4'
            min: '100m'
          storage:
            data:
              capacity: 2Gi
----
