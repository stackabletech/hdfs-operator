= Resources
:description: Configure HDFS storage with PersistentVolumeClaims for custom data volumes and multiple disk types. Set resource requests for HA setups in Kubernetes.

== Storage for data volumes

You can mount volumes where data is stored by specifying https://kubernetes.io/docs/concepts/storage/persistent-volumes[PersistentVolumeClaims] for each individual role group.

In case nothing is configured in the custom resource for a certain role group, each Pod has one volume mount with `10Gi` capacity and storage type `Disk`:

[source,yaml]
----
dataNodes:
  roleGroups:
    default:
      config:
        resources:
          storage:
            data:
              count: 1
              capacity: 10Gi
              hdfsStorageType: Disk
----

These defaults can be overridden individually:

[source,yaml]
----
dataNodes:
  roleGroups:
    default:
      config:
        resources:
          storage:
            data:
              capacity: 128Gi
----

In the above example, all DataNodes in the default group store data (the location of `dfs.datanode.name.dir`) on a `128Gi` volume.

=== Multiple storage volumes

Datanodes can have multiple disks attached to increase the storage size as well as speed.
They can be of different type, e.g. HDDs or SSDs.

You can configure multiple https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims[PersistentVolumeClaims] (PVCs) for the datanodes as follows:

[source,yaml]
----
dataNodes:
  roleGroups:
    default:
      config:
        resources:
          storage:
            my-disks:
              count: 3
              capacity: 12Ti
              hdfsStorageType: Disk
            my-ssds:
              count: 2
              capacity: 5Ti
              storageClass: premium-ssd
              hdfsStorageType: SSD
            # The default "data" PVC is still created.
            # If this is not desired then the count must be set to 0.
            data:
              count: 0
----

This creates the following PVCs:

1. `my-disks-hdfs-datanode-default-0` (12Ti)
2. `my-disks-1-hdfs-datanode-default-0` (12Ti)
3. `my-disks-2-hdfs-datanode-default-0` (12Ti)
4. `my-ssds-hdfs-datanode-default-0` (5Ti)
5. `my-ssds-1-hdfs-datanode-default-0` (5Ti)

By configuring and using a dedicated https://kubernetes.io/docs/concepts/storage/storage-classes/[StorageClass] you can configure your HDFS to use local disks attached to Kubernetes nodes.

[NOTE]
====
You might need to re-create the StatefulSet to apply the new PVC configuration because of https://github.com/kubernetes/kubernetes/issues/68737[this Kubernetes issue].
You can delete the StatefulSet using `kubectl delete statefulsets --cascade=orphan <statefulset>`.
The hdfs-operator recreates the StatefulSet automatically.
====

== Resource Requests

include::home:concepts:stackable_resource_requests.adoc[]

A minimal HA setup consisting of 3 journalnodes, 2 namenodes and 2 datanodes has the following https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/[resource requirements]:

* `2950m` CPU request
* `8300m` CPU limit
* `6528m` memory request and limit
* `27648Mi` persistent storage

Corresponding to the values above, the operator uses the following resource defaults:

[source,yaml]
----
spec:
  journalNodes:
    config:
      resources:
        cpu:
          min: 100m
          max: 400m
        memory:
          limit: 512Mi
        storage:
          data:
            capacity: 1Gi
  nameNodes:
    config:
      resources:
        cpu:
          min: 250m
          max: 1000m
        memory:
          limit: 1024Mi
        storage:
          data:
            capacity: 2Gi
  dataNodes:
    config:
      resources:
        cpu:
          min: 100m
          max: 400m
        memory:
          limit: 512Mi
        storage:
          data:
            capacity: 10Gi
----
