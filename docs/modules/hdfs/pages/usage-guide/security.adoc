= Security

== Authentication
Currently the only supported authentication mechanism is Kerberos, which is disabled by default.
For Kerberos to work a Kerberos KDC is needed, which the users needs to provide.
The xref:home:secret-operator:secretclass.adoc#backend-kerberoskeytab[secret-operator documentation] states which kind of Kerberos servers are supported and how they can be configured.

IMPORTANT: Kerberos is supported starting from HDFS version 3.3.x

=== 1. Prepare Kerberos server
To configure HDFS to use Kerberos you first need to collect information about your Kerberos server, e.g. hostname and port.
Additionally you need a service-user, which the secret-operator uses to create create principals for the HDFS services.

=== 2. Create Kerberos SecretClass
Afterwards you need to enter all the needed information into a SecretClass, as described in xref:home:secret-operator:secretclass.adoc#backend-kerberoskeytab[secret-operator documentation].
The following guide assumes you have named your SecretClass `kerberos-hdfs`.

=== 3. Configure HDFS to use SecretClass
The last step is to configure your HdfsCluster to use the newly created SecretClass.

[source,yaml]
----
spec:
  clusterConfig:
    authentication:
      tlsSecretClass: tls # Optional, defaults to "tls"
      kerberos:
        secretClass: kerberos-hdfs # Put your SecretClass name in here
----

The `kerberos.secretClass` is used to give HDFS the possibility to request keytabs from the secret-operator.

The `tlsSecretClass` is needed to request TLS certificates, used e.g. for the Web UIs.


=== 4. Verify that Kerberos authentication is required
Use `stackablectl stacklet list` to get the endpoints where the HDFS namenodes are reachable.
Open the link (note that the namenode is now using https).
You should see a Web UI similar to the following:

image:hdfs_webui_kerberos.png[]

The important part is

> Security is on.

You can also shell into the namenode and try to access the file system:
`kubectl exec -it hdfs-namenode-default-0 -c namenode -- bash -c 'kdestroy && bin/hdfs dfs -ls /'`

You should get the error message `org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]`.

=== 5. Access HDFS
In case you want to access your HDFS it is recommended to start up a client Pod that connects to HDFS, rather than shelling into the namenode.
We have an https://github.com/stackabletech/hdfs-operator/blob/main/tests/templates/kuttl/kerberos/20-access-hdfs.yaml.j2[integration test] for this exact purpose, where you can see how to connect and get a valid keytab.

== Authorization
For authorization we developed https://github.com/stackabletech/hdfs-utils[hdfs-utils], which contains an OPA authorizer and group mapper.
This matches our general xref:home:concepts:opa.adoc[] mechanisms.

IMPORTANT: It is recommended to enable Kerberos when doing Authorization, as otherwise you don't have any security measures at all.

In order to use the authorizer you need a ConfigMap containing a rego rule and reference that in your HDFS cluster.
In addition to this you need a OpaCluster that serves the rego rules - this guide assumes it's called `opa`.

[source,rego]
----
include::example$usage-guide/hdfs-regorules.yaml[]
----

Reference it as follows in your HdfsCluster:

[source,yaml]
----
spec:
  clusterConfig:
    authentication:
      tlsSecretClass: tls
      kerberos:
        secretClass: kerberos-$NAMESPACE
    authorization:
      opa:
        configMapName: opa
        package: hdfs
----

=== Username extraction
HDFS allows you to configure how the `shortUsername` is extracted from the Kerberos principal using https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html#Mapping_from_Kerberos_principals_to_OS_user_accounts[`hadoop.security.auth_to_local`].
Stackable does not use a custom mechanism, instead we stick to the default setup.
The authorizer sends the whole UserGroupInformation (including the `shortUserName` *and* the full `userName`) to OPA, which is perfect.
The GroupMapper interface however only gives the `shortUsername` while https://github.com/apache/hadoop/blob/a897e745f598ef05fc0c253b2a776100e48688d2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/GroupMappingServiceProvider.java#L45[asking for the group memberships].

Because of this limitation we tried to map the principals from Kerberos 1:1 to the HDFS `shortUserName`, so the GroupMapper has access to the full `userName`. However this did not work, as HDFS only allows "simple usernames", https://github.com/apache/hadoop/blob/8378ab9f92c72dc6164b62f7be71826fd750dba4/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java#L348[which are not allowed to contain a `/` or `@`].
Because of this limitation, the GroupMapper ignores the `shortUsername` passed to the function and instead calls to `UserGroupInformation.getCurrentUser()`,  uses `.getUserName()` to extract the full `userName` and uses that to lookup the group memberships.

== Wire encryption
In case Kerberos is enabled, `Privacy` mode is used for best security.
Wire encryption without Kerberos as well as https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html#Data_confidentiality[other wire encryption modes] are *not* supported.
