= Security

== Authentication
Currently the only supported authentication mechanism is Kerberos, which is disabled by default.
For Kerberos to work a Kerberos KDC is needed, which the users needs to provide.
The xref:home:secret-operator:secretclass.adoc#backend-kerberoskeytab[secret-operator documentation] states which kind of Kerberos servers are supported and how they can be configured.

IMPORTANT: Kerberos is supported starting from HDFS version 3.3.x

=== 1. Prepare Kerberos server
To configure HDFS to use Kerberos you first need to collect information about your Kerberos server, e.g. hostname and port.
Additionally you need a service-user, which the secret-operator uses to create create principals for the HDFS services.

=== 2. Create Kerberos SecretClass
Afterwards you need to enter all the needed information into a SecretClass, as described in xref:home:secret-operator:secretclass.adoc#backend-kerberoskeytab[secret-operator documentation].
The following guide assumes you have named your SecretClass `kerberos-hdfs`.

=== 3. Configure HDFS to use SecretClass
The last step is to configure your HdfsCluster to use the newly created SecretClass.

[source,yaml]
----
spec:
  clusterConfig:
    authentication:
      tlsSecretClass: tls # Optional, defaults to "tls"
      kerberos:
        secretClass: kerberos-hdfs # Put your SecretClass name in here
----

The `kerberos.secretClass` is used to give HDFS the possibility to request keytabs from the secret-operator.

The `tlsSecretClass` is needed to request TLS certificates, used e.g. for the Web UIs.


=== 4. Verify that Kerberos authentication is required
Use `stackablectl stacklet list` to get the endpoints where the HDFS namenodes are reachable.
Open the link (note that the namenode is now using https).
You should see a Web UI similar to the following:

image:hdfs_webui_kerberos.png[]

The important part is

> Security is on.

You can also shell into the namenode and try to access the file system:
`kubectl exec -it hdfs-namenode-default-0 -c namenode -- bash -c 'kdestroy && bin/hdfs dfs -ls /'`

You should get the error message `org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]`.

=== 5. Access HDFS
In case you want to access your HDFS it is recommended to start up a client Pod that connects to HDFS, rather than shelling into the namenode.
We have an https://github.com/stackabletech/hdfs-operator/blob/main/tests/templates/kuttl/kerberos/20-access-hdfs.yaml.j2[integration test] for this exact purpose, where you can see how to connect and get a valid keytab.

== Authorization
For authorization we developed https://github.com/stackabletech/hdfs-utils[hdfs-utils], which contains an OPA authorizer and group mapper.
This matches our general xref:home:concepts:opa.adoc[] mechanisms.

IMPORTANT: It is recommended to enable Kerberos when doing Authorization, as otherwise you don't have any security measures at all.

In order to use the authorizer you need a ConfigMap containing a rego rule and reference that in your HDFS cluster.
In addition to this you need a OpaCluster that serves the rego rules - this guide assumes it's called `opa`.

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: hdfs-opa-bundle
  labels:
    opa.stackable.tech/bundle: "true"
data:
  hdfs.rego: |
    package hdfs

    import rego.v1

    default allow = false

    # HDFS authorizer
    allow if {
        some acl in acls
        matches_identity(input.callerUgi.shortUserName, acl.identity)
        matches_resource(input.path, acl.resource)
        action_sufficient_for_operation(acl.action, input.operationName)
    }

    # HDFS group mapper (this returns a list of strings)
    groups := {group |
        raw = groups_for_user[input.username][_]
        # Keycloak groups have trailing slashes
        group := trim_prefix(raw, "/")
    }

    # Identity mentions the user explicitly
    matches_identity(user, identity) if {
        identity == concat("", ["user:", user])
    }

    # Identity mentions group the user is part of
    matches_identity(user, identity) if {
        some group in groups_for_user[user]
        identity == concat("", ["group:", group])
    }

    # Resource mentions the file explicitly
    matches_resource(file, resource) if {
        resource == concat("", ["hdfs:file:", file])
    }

    # Resource mentions the directory explicitly
    matches_resource(file, resource) if {
        trim_suffix(resource, "/") == concat("", ["hdfs:dir:", file])
    }

    # Resource mentions a folder higher up the tree, which will will grant access recursively
    matches_resource(file, resource) if {
        startswith(resource, "hdfs:dir:/")
        # directories need to have a trailing slash
        endswith(resource, "/")
        startswith(file, trim_prefix(resource, "hdfs:dir:"))
    }

    action_sufficient_for_operation(action, operation) if {
        action_hierarchy[action][_] == action_for_operation[operation]
    }

    action_hierarchy := {
        "full": ["full", "rw", "ro"],
        "rw": ["rw", "ro"],
        "ro": ["ro"],
    }

    # To get a (hopefully complete) list of actions run "ack 'String operationName = '" in the hadoop source code
    action_for_operation := {
        # The "rename" operation will be actually called on both - the source and the target location.
        # Because of this you need to have rw permissions on the source and target file - which is desired

        "abandonBlock": "rw",
        "addCacheDirective": "rw",
        "addCachePool": "full",
        "addErasureCodingPolicies": "full",
        "allowSnapshot": "full",
        "append": "rw",
        "cancelDelegationToken": "ro",
        "checkAccess": "ro",
        "clearQuota": "full",
        "clearSpaceQuota": "full",
        "completeFile": "rw",
        "computeSnapshotDiff": "full",
        "concat": "rw",
        "contentSummary": "ro",
        "create": "rw",
        "createEncryptionZone": "full",
        "createSnapshot": "full",
        "createSymlink": "rw",
        "delete": "rw",
        "deleteSnapshot": "full",
        "disableErasureCodingPolicy": "full",
        "disallowSnapshot": "full",
        "enableErasureCodingPolicy": "full",
        "finalizeRollingUpgrade": "full",
        "fsck": "full",
        "fsckGetBlockLocations": "full",
        "fsync": "rw",
        "gcDeletedSnapshot": "full",
        "getAclStatus": "ro",
        "getAdditionalBlock": "ro",
        "getAdditionalDatanode": "ro",
        "getDelegationToken": "ro",
        "getECTopologyResultForPolicies": "ro",
        "getErasureCodingCodecs": "ro",
        "getErasureCodingPolicies": "ro",
        "getErasureCodingPolicy": "ro",
        "getEZForPath": "ro",
        "getfileinfo": "ro",
        "getPreferredBlockSize": "ro",
        "getStoragePolicy": "ro",
        "getXAttrs": "ro",
        "isFileClosed": "ro",
        "listCacheDirectives": "ro",
        "listCachePools": "ro",
        "listCorruptFileBlocks": "ro",
        "listEncryptionZones": "ro",
        "listOpenFiles": "ro",
        "listReencryptionStatus": "ro",
        "ListSnapshot": "ro", # Yeah, this really starts with a capital letter
        "listSnapshottableDirectory": "ro",
        "listStatus": "ro",
        "listXAttrs": "ro",
        "mkdirs": "rw",
        "modifyAclEntries": "full",
        "modifyCacheDirective": "rw",
        "modifyCachePool": "full",
        "open": "ro",
        "queryRollingUpgrade": "ro",
        "quotaUsage": "ro",
        "recoverLease": "full",
        "reencryptEncryptionZone": "full",
        "removeAcl": "full",
        "removeAclEntries": "full",
        "removeCacheDirective": "rw",
        "removeCachePool": "full",
        "removeDefaultAcl": "full",
        "removeErasureCodingPolicy": "full",
        "removeXAttr": "rw",
        "rename": "rw",
        "renameSnapshot": "full",
        "renewDelegationToken": "ro",
        "satisfyStoragePolicy": "full",
        "setAcl": "full",
        "setErasureCodingPolicy": "full",
        "setOwner": "full",
        "setPermission": "full",
        "setQuota": "full",
        "setReplication": "full",
        "setSpaceQuota": "full",
        "setStoragePolicy": "full",
        "setTimes": "rw",
        "setXAttr": "rw",
        "startRollingUpgrade": "full",
        "truncate": "rw",
        "unsetErasureCodingPolicy": "full",
        "unsetStoragePolicy": "full",
    }

    # Actions I think are only relevant for the whole filesystem, and not specific to a file or directory
    admin_actions := {
        "checkRestoreFailedStorage": "ro",
        "datanodeReport": "ro",
        "disableRestoreFailedStorage": "full",
        "enableRestoreFailedStorage": "full",
        "finalizeUpgrade": "rw",
        "getDatanodeStorageReport": "ro",
        "metaSave": "ro",
        "monitorHealth": "ro",
        "refreshNodes": "rw",
        "rollEditLog": "rw",
        "saveNamespace": "full",
        "setBalancerBandwidth": "rw",
        "slowDataNodesReport": "ro",
        "transitionToActive": "full",
        "transitionToObserver": "full",
        "transitionToStandby": "full",
    }

    groups_for_user := {"admin": ["admins"], "alice": ["developers"], "bob": []}

    acls := [
        {
            "identity": "group:admins",
            "action": "full",
            "resource": "hdfs:dir:/",
        },
        {
            "identity": "group:developers",
            "action": "rw",
            "resource": "hdfs:dir:/developers/",
        },
        {
            "identity": "group:developers",
            "action": "ro",
            "resource": "hdfs:dir:/developers-ro/",
        },
        {
            "identity": "user:alice",
            "action": "rw",
            "resource": "hdfs:dir:/alice/",
        },
        {
            "identity": "user:bob",
            "action": "rw",
            "resource": "hdfs:dir:/bob/",
        },
        {
            "identity": "user:bob",
            "action": "ro",
            "resource": "hdfs:dir:/developers/",
        },
        {
            "identity": "user:bob",
            "action": "rw",
            "resource": "hdfs:file:/developers/file-from-bob",
        },
    ]
----

Reference it as follows in your HdfsCluster:

[source,yaml]
----
spec:
  clusterConfig:
    authentication:
      tlsSecretClass: tls
      kerberos:
        secretClass: kerberos-$NAMESPACE
    authorization:
      opaAuthorization:
        configMapName: opa
        package: hdfs
      opaGroupMapping:
        configMapName: opa
        package: hdfs
----

== Wire encryption
In case Kerberos is enabled, `Privacy` mode is used for best security.
Wire encryption without Kerberos as well as https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html#Data_confidentiality[other wire encryption modes] are *not* supported.
